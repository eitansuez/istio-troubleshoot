{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":"<p>The materials on this portal help you explore ways of debugging and troubleshooting Istio.</p> <p>You will learn about different commands and tools for ascertaining the state of your mesh and otherwise troubleshoot a particular issue.</p>"},{"location":"controlplane-health/","title":"Control plane Health","text":"<p>Part of ensuring the health of the mesh includes ensuring the health of the control plane. That would be <code>istiod</code> and all of the activities it performs.</p>"},{"location":"controlplane-health/#logs","title":"Logs","text":"<p>We can tail the logs for <code>istiod</code>:</p> <pre><code>kubectl logs --follow -n istio-system deploy/istiod\n</code></pre> <p>Among other things, you should see lines reflecting activity such as pushing configuration to sidecars for different workloads.</p>"},{"location":"controlplane-health/#configuring-log-levels","title":"Configuring log levels","text":"<p>Similar to the <code>istioctl proxy-config log</code> command, Istio also provides a command for configuring logging for the controlplane.</p> <p>Inspect the log level for each logger with:</p> <pre><code>istioctl admin log\n</code></pre> <p>Alter the log level for a logger with the <code>--level</code> flag:</p> <pre><code>istioctl admin log --level ads:warn\n</code></pre> <p>Unlike the log command for Envoys, the command will not echo back the update state of the loggers.</p> <p>To reset the log level, use the <code>--reset</code> flag:</p> <pre><code>istioctl admin log --reset\n</code></pre>"},{"location":"controlplane-health/#dashboards","title":"Dashboards","text":"<p>In Grafana, the principal dashboard for monitoring the control plane is, as its name implies, the Control Plane dashboard.</p> <pre><code>istioctl dashboard grafana\n</code></pre> <p>On the control plane dashboard, you will find information pertaining to:</p> <ul> <li>Resource usage for <code>istiod</code></li> <li>Frequency of pushes (of Envoy configurations)</li> <li>\"Pilot\" errors</li> <li>The number of services that Istio is connected to</li> <li>Number of connections to Envoys</li> <li>Times that connections were established</li> <li>The size of XDS requests and responses</li> </ul> <p>In addition, the Performance dashboard also contains (or repeats) resource usage for <code>istiod</code>, in addition to gateways and proxies.</p>"},{"location":"controlplane-health/#what-affects-istios-resource-utilization","title":"What affects Istio's resource utilization?","text":"<p>Istio recently switched to using the Delta XDS protocol, which is more efficient in comparison to the \"state of the world\" version, in terms of the amount of configuration that has to be communicated to each Envoy.</p> <p>The factors listed below affect Istio's resource usage:</p> <ul> <li>the number of running workloads and services</li> <li>the frequency of deployment changes</li> <li>the frequency of mesh configuration changes</li> <li>the scope of service discoverability</li> </ul> <p>The last item on the list is something we can do something about.</p> <p>By default, Istio programs all workloads with information about every other workload.</p> <p>In our sandbox environment, when we realize, for example, that:</p> <ul> <li>httpbin doesn't need to know about all of the bookinfo services</li> <li><code>productpage</code> is the main service that calls most other <code>bookinfo</code> services</li> </ul> <p>Through the Sidecar resource, we can communicate to Istio what services each workload needs to know about.</p> <p>This can go a long way to reducing Istio's footprint.</p> <p>Here is an example set of Sidecar resources that fine-tune the list of services that each deployment needs to know about:</p> <pre><code>---\napiVersion: networking.istio.io/v1beta1\nkind: Sidecar\nmetadata:\n  name: productpage-sidecar\n  namespace: default\nspec:\n  workloadSelector:\n    labels:\n      app: productpage\n  egress:\n  - hosts:\n    - \"./reviews.default.svc.cluster.local\"\n    - \"./details.default.svc.cluster.local\"\n    - \"istio-system/*\"\n---\napiVersion: networking.istio.io/v1beta1\nkind: Sidecar\nmetadata:\n  name: reviews-v1-sidecar\n  namespace: default\nspec:\n  workloadSelector:\n    labels:\n      app: reviews\n      version: v1\n  egress:\n  - hosts:\n    - \"istio-system/*\"\n---\napiVersion: networking.istio.io/v1beta1\nkind: Sidecar\nmetadata:\n  name: reviews-v2-sidecar\n  namespace: default\nspec:\n  workloadSelector:\n    labels:\n      app: reviews\n      version: v2\n  egress:\n  - hosts:\n    - \"./ratings.default.svc.cluster.local\"\n    - \"istio-system/*\"\n---\napiVersion: networking.istio.io/v1beta1\nkind: Sidecar\nmetadata:\n  name: reviews-v3-sidecar\n  namespace: default\nspec:\n  workloadSelector:\n    labels:\n      app: reviews\n      version: v3\n  egress:\n  - hosts:\n    - \"./ratings.default.svc.cluster.local\"\n    - \"istio-system/*\"\n---\napiVersion: networking.istio.io/v1beta1\nkind: Sidecar\nmetadata:\n  name: details-sidecar\n  namespace: default\nspec:\n  workloadSelector:\n    labels:\n      app: details\n  egress:\n  - hosts:\n    - \"istio-system/*\"\n---\napiVersion: networking.istio.io/v1beta1\nkind: Sidecar\nmetadata:\n  name: ratings-sidecar\n  namespace: default\nspec:\n  workloadSelector:\n    labels:\n      app: ratings\n  egress:\n  - hosts:\n    - \"istio-system/*\"\n---\napiVersion: networking.istio.io/v1beta1\nkind: Sidecar\nmetadata:\n  name: httpbin-sidecar\n  namespace: default\nspec:\n  workloadSelector:\n    labels:\n      app: httpbin\n  egress:\n  - hosts:\n    - \"istio-system/*\"\n</code></pre> <p>Apply the sidecars to your cluster:</p> <pre><code>kubectl apply -f artifacts/cp-health/sidecars.yaml\n</code></pre> <p>The control plane dashboard will show an XDS push that updates the sidecar configurations accordingly.</p> <p>In this sandbox environment with so few services, this hardly makes a difference.</p>"},{"location":"data-plane/","title":"The data plane","text":"<p>In the previous exploration we looked at issues relating to misconfiguration.</p> <p>In this exploration, we investigate issues with the data plane:  everything is configured correctly, but some traffic flow isn't functioning, and we need to find out why.</p> <p>Ingress is configured for the <code>bookinfo</code> application, routing requests to the <code>productpage</code> destination.</p> <p>Assuming the local cluster deployed with k3d in setup, the ingress gateway is reachable on localhost, port 80:</p> <pre><code>export GATEWAY_IP=localhost\n</code></pre>"},{"location":"data-plane/#no-healthy-upstream-uh","title":"No Healthy Upstream (UH)","text":"<p>What if for some reason the backing workload is not accessible?</p> <p>Simulate this situation by scaling the <code>productpage-v1</code> deployment to zero replicas:</p> <pre><code>kubectl scale deploy productpage-v1 --replicas 0\n</code></pre> <p>In one terminal, tail the logs of the ingress gateway:</p> <pre><code>kubectl logs --follow -n istio-system -l istio=ingressgateway\n</code></pre> <p>In another terminal, send a request in through the ingress gateway:</p> <pre><code>curl http://$GATEWAY_IP/productpage\n</code></pre> <p>In the logs you should see the following line:</p> <pre><code>\"GET /productpage HTTP/1.1\" 503 UH no_healthy_upstream - \"-\" 0 19 0 - \"10.42.0.1\" \"curl/8.7.1\" \"c4c58af1-2066-4c45-affb-d1345d32fc66\" \"localhost\" \"-\" outbound|9080||productpage.default.svc.cluster.local - 10.42.0.7:8080 10.42.0.1:60667 - -\n</code></pre> <p>Note the UH response flag:  No Healthy Upstream.</p> <p>These response flags clearly communicate to the operator the reason for which the request did not succeed.</p>"},{"location":"data-plane/#no-route-found-nr","title":"No Route Found (NR)","text":"<p>As another example, make a request to a route that does not match any routing rules in the virtual service:</p> <pre><code>curl http://$GATEWAY_IP/productpages\n</code></pre> <p>The log entry responds with a 404 \"NR\", for \"No Route Found\":</p> <pre><code>\"GET /productpages HTTP/1.1\" 404 NR route_not_found - \"-\" 0 0 0 - \"10.42.0.1\" \"curl/8.7.1\" \"2606aaa9-8c5c-4987-9ba7-86b89f901d34\" \"localhost\" \"-\" - - 10.42.0.7:8080 10.42.0.1:13819 - -\n</code></pre>"},{"location":"data-plane/#upstreamretrylimitexceeded-urx","title":"UpstreamRetryLimitExceeded (URX)","text":"<p>Delete the <code>bookinfo</code> Gateway and VirtualService resources:</p> <pre><code>kubectl delete -f artifacts/mesh-config/bookinfo-gateway.yaml\n</code></pre> <p>In its place, configure ingress for the <code>httpbin</code> workload:</p> <pre><code>kubectl apply -f artifacts/data-plane/httpbin-gateway.yaml\n</code></pre> <pre><code>---\napiVersion: networking.istio.io/v1alpha3\nkind: Gateway\nmetadata:\n  name: httpbin-gateway\nspec:\n  selector:\n    istio: ingressgateway\n  servers:\n  - port:\n      number: 80\n      name: http\n      protocol: HTTP\n    hosts:\n    - \"*\"\n---\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: httpbin\nspec:\n  hosts:\n  - \"*\"\n  gateways:\n  - httpbin-gateway\n  http:\n  - route:\n    - destination:\n        host: httpbin\n        port:\n          number: 8000\n    retries:\n      attempts: 3\n      retryOn: gateway-error,connect-failure,refused-stream\n</code></pre> <p>The VirtualService is configured with three retry attempts in the event of a 503 response.</p> <p>Call the <code>httpbin</code> endpoint that returns a 503:</p> <pre><code>curl -v http://$GATEWAY_IP/status/503\n</code></pre> <p>The Envoy gateway logs will show the response flag URX:  UpstreamRetryLimitExceeded:</p> <pre><code>\"GET /status/503 HTTP/1.1\" 503 URX via_upstream - \"-\" 0 0 120 119 \"10.42.0.1\" \"curl/8.7.1\" \"dcb3b100-e296-4031-8f45-1234d20b0f20\" \"localhost\" \"10.42.0.9:8080\" outbound|8000||httpbin.default.svc.cluster.local 10.42.0.7:38902 10.42.0.7:8080 10.42.0.1:51761 - -\n</code></pre> <p>That is, the gateway got a 503, retried the request up to three times, and then gave up.</p> <p>Envoy's response flags provide insight into why a request to a target destination workload might have failed.</p>"},{"location":"data-plane/#sidecar-logs","title":"Sidecar logs","text":"<p>We are not restricted to inspecting the logs of the ingress gateway.  We can also check the logs of the Envoy sidecars.</p> <p>Tail the logs for the sidecar of the <code>httpbin</code> destination workload:</p> <pre><code>kubectl logs --follow deploy/httpbin -c istio-proxy\n</code></pre> <p>Repeat the call to the <code>httpbin</code> \"503\" endpoint:</p> <pre><code>curl -v http://$GATEWAY_IP/status/503\n</code></pre> <p>You will see evidence of four inbound requests received by the sidecar, i.e. three retry attempts.</p>"},{"location":"data-plane/#log-levels","title":"Log levels","text":"<p>The log level for any Envoy proxy can be either displayed or configured with the proxy-config log command.</p> <p>Envoy has many loggers.  The log level for each logger can be configured independently.</p> <p>For example, let us target the Istio ingress gateway deployment.</p> <p>To view the log levels for each logger, run:</p> <pre><code>istioctl proxy-config log -n istio-system deploy/istio-ingressgateway\n</code></pre> <p>The log levels are: <code>trace</code>, <code>debug</code>, <code>info</code>, <code>warning</code>, <code>error</code>, <code>critical</code>, and <code>off</code>.</p> <p>To set the log level for, say the wasm logger, to <code>info</code>:</p> <pre><code>istioctl proxy-config log -n istio-system deploy/istio-ingressgateway --level wasm:info\n</code></pre> <p>This can be useful for debugging wasm extensions.</p> <p>The output displays the updated logging levels for every logger for that Envoy instance.</p> <p>Log levels can be reset for all loggers with the <code>--reset</code> flag:</p> <pre><code>istioctl proxy-config log -n istio-system deploy/istio-ingressgateway --reset\n</code></pre>"},{"location":"dataplane-health/","title":"Data plane Health","text":""},{"location":"dataplane-health/#objective","title":"Objective","text":"<p>To get acquainted with observability tools, how they help us understand the behavior of our system, with a focus on the health of the data plane.</p>"},{"location":"dataplane-health/#introduction","title":"Introduction","text":"<p>The data plane consists of the gateways, sidecars and workloads through which traffic flows.</p> <p>A healthy data plane should be available, responsive, there should be no (or few) errors.</p> <p>We focus on RED metrics:  Requests, Errors, and Durations</p>"},{"location":"dataplane-health/#setup","title":"Setup","text":"<p>Splitting traffic to the <code>reviews</code> service between subsets <code>v1</code> and <code>v3</code> will allow us to observe traffic distribution:</p> <pre><code>kubectl apply -f istio-1.22.0/samples/bookinfo/networking/virtual-service-reviews-50-v3.yaml\n</code></pre>"},{"location":"dataplane-health/#kiali","title":"Kiali","text":"<p>Kiali is a monitoring solution designed specifically for Istio. It is an open-source project, and is primarily known for its call graph visualizations.</p> <p>Launch the Kiali dashboard:</p> <pre><code>istioctl dashboard kiali\n</code></pre> <p>In the navigation bar, select \"Traffic Graph\", and point the namespace pulldown menu to the <code>default</code> namespace.</p> <p>A basic visualization will appear.  The call graph is constructed from live traffic flows within the system.</p> <p>We can decorate this visualization with additional information.  We can turn on traffic animations, security information, traffic rate (requests per second), traffic distribution (%), request and response throughput (bytes per second), and response times (ms).  The arrows between services are color-code to signal whether requests are currently succeeding or failing.</p> <p>You should see a 50/50 traffic split between <code>v1</code> and <code>v3</code> subsets of the <code>reviews</code> service, low latencies, no errors, acknowledgements that the communication between all services is mutual-tls enabled.</p> <p>We also gain an understanding of the call graph:  the interactions between services, and their respective volumes.</p> <p>Kiali also allows us to inspect and modify Istio custom resources, look at specific services, workloads and their metrics, and will produce call graphs in the context of (from the perspective of) a specific service or workload.</p>"},{"location":"dataplane-health/#zipkin","title":"Zipkin","text":"<p>The focus of the Zipkin dashboard is to uncover errors and latency issues in a call graph.</p> <pre><code>istioctl dashboard zipkin\n</code></pre> <p>Search for traces involving the <code>productpage</code> serviceName, click \"Run Query\". We can also look for traces with durations above a certain minimum.</p> <p>Select a trace by clicking the \"Show\" button. The resulting visualization helps answer questions such as:</p> <ul> <li>What services were called?</li> <li>How much time was spent inside each service?</li> <li>How much time is spent on the network?</li> <li>Are multiple services called in parallel or serially?</li> </ul> <p>We can also look at metadata that was collected with each trace and with each span, details of the requests, the HTTP method, the url, the upstream or downstream peer, etc..</p>"},{"location":"dataplane-health/#grafana","title":"Grafana","text":"<p>In our setup, metrics are exposed by Envoys, and collected by Prometheus. Grafana has Prometheus pre-configured as its data source. The dashboards you will explore expose those metrics in an intelligible way.</p> <pre><code>istioctl dashboard grafana\n</code></pre>"},{"location":"dataplane-health/#the-mesh-dashboard","title":"The mesh dashboard","text":"<p>Start with the mesh dashboard, designed to provide a high-level overview of the state of the mesh:</p> <ul> <li>the global request volume</li> <li>global success rate</li> <li>4xx and 5xx errors</li> <li>the number and kind of Istio resources currently applied to the cluster:  Gateways, VirtuaServices, DestinationRules, Authorization Policies, etc..</li> </ul> <p>For each service we also get their salient metrics:  request volume, latency distribution, and success rate.</p>"},{"location":"dataplane-health/#service-dashboards","title":"Service dashboards","text":"<p>We can drill down a step further by looking at the service dashboard. Select the service to monitor from the pulldown menu at the top of the page.</p> <p>The \"General\" panel contains the RED metrics for that service:  request volume, success rate, and request durations (percentiles).</p> <p>Under \"Client workloads\" we can see a breakdown of requests to this service by client.  In our case this is not so informative since most services have a single distinct client:  productpage is called by the ingressgateway, details is called by productpage, etc..</p> <p>The third panel, \"Service Workloads\", breaks down the metrics by backing workloads.  For example, the <code>reviews</code> service is backed by one instance of <code>reviews-v1</code> and one instance of <code>reviews-v3</code>. Istio service &amp; workload Grafana dashboards</p>"},{"location":"dataplane-health/#workload-dashboards","title":"Workload dashboards","text":"<p>At the most detailed level, the workload dashboards allow us to look at metrics for a specific workload.  For example look at the workload <code>reviews-v3</code>.</p> <p>We can get the RED metrics for this workload to the exclusion of other workloads. We see incoming requests from the <code>productpage</code> service, and outbound requests to the <code>ratings</code> service.</p>"},{"location":"dataplane-health/#data-plane-metrics","title":"Data plane metrics","text":"<p>When monitoring the data plane, look for:</p> <ul> <li>High 4xx error rate</li> <li>High 5xx error rate</li> <li>High request latency</li> <li>Latency 99 percentile</li> </ul>"},{"location":"dataplane-health/#todo","title":"TODO","text":"<ul> <li>Adding fault injection and show what error scenarios look like.</li> <li>The Prometheus dashboard and PromQL queries</li> </ul>"},{"location":"install/","title":"Installation","text":""},{"location":"install/#objective","title":"Objective","text":"<p>To explore troubleshooting and diagnostic commands associated with the installation of Istio.</p>"},{"location":"install/#precheck","title":"Precheck","text":"<p>The precheck command helps ascertain that Istio can be installed or upgraded on the current Kubernetes cluster:</p> <pre><code>istioctl x precheck\n</code></pre>"},{"location":"install/#install-istio","title":"Install Istio","text":"<p>Istio is often installed with the <code>istioctl</code> CLI in sandbox environments.</p> <p><code>helm</code> is typically the method preferred for QA, staging, and production environments.</p> <p>Use the Istio CLI to install Istio with the default configuration profile, which deploys <code>istiod</code> and the Istio ingress gateway component:</p> <pre><code>istioctl install -f artifacts/install/trace-config.yaml\n</code></pre> <p>Above, we also reference an installation configuration that configures distributed tracing.</p> <pre><code>---\napiVersion: install.istio.io/v1alpha1\nkind: IstioOperator\nspec:\n  profile: default\n  meshConfig:\n    enableTracing: true\n    defaultConfig:\n      tracing:\n        sampling: 100.0\n    extensionProviders:\n    - name: zipkin\n      zipkin:\n        service: zipkin.istio-system.svc.cluster.local\n        port: 9411\n</code></pre>"},{"location":"install/#is-istio-installed-properly","title":"Is istio installed properly?","text":"<p>Given an environment with Istio installed, we can verify the installation with the verify-install command:</p> <pre><code>istioctl verify-install\n</code></pre>"},{"location":"install/#what-version-of-istio-am-i-running","title":"What version of Istio am I running?","text":"<p>The version command provides version information for the CLI, the control plane, and the data plane (proxies):</p> <pre><code>istioctl version\n</code></pre> <p>Question</p> <p>Part of the output from <code>istioctl version</code> is:</p> <pre><code>data plane version: 1.21.2 (1 proxies)\n</code></pre> <p>Can you explain what this means?  What proxies are being referred to?</p>"},{"location":"install/#access-logging","title":"Access Logging","text":"<p>When using the <code>default</code> configuration profile, Envoy sidecars and gateways are not default-configured with access logging to standard output.</p> <p>We can enable Envoy Access logging using Istio's Telemetry API.</p> <p>Apply the following resource to your Kubernetes cluster:</p> <pre><code>kubectl apply -f artifacts/install/telemetry.yaml\n</code></pre> <pre><code>---\napiVersion: telemetry.istio.io/v1alpha1\nkind: Telemetry\nmetadata:\n  name: mesh-default\n  namespace: istio-system\nspec:\n  accessLogging:\n  - providers:\n    - name: envoy\n  tracing:\n  - providers:\n    - name: zipkin\n</code></pre> <p>Note that we also set the tracing provider to zipkin.  More on that later.</p>"},{"location":"mesh-configurations/","title":"Mesh Configurations","text":"<p>This section explores troubleshooting mesh configurations, and what tools are at your disposal to validate and diagnose configuration-related issues.</p>"},{"location":"mesh-configurations/#deploy-the-bookinfo-sample-application","title":"Deploy the <code>bookinfo</code> sample application","text":"<p>Take a moment to familiarize yourself with the Istio sample application <code>bookinfo</code>.</p> <p>Verify that the <code>default</code> namespace is labeled for sidecar injection:</p> <pre><code>kubectl get ns -Listio-injection\n</code></pre> <p>If it isn't labeled, the first step is to label it:</p> <pre><code>kubectl label ns default istio-injection=enabled\n</code></pre> <p>Deploy the bookinfo sample application to the <code>default</code> namespace:</p> <pre><code>kubectl apply -f istio-1.22.0/samples/bookinfo/platform/kube/bookinfo.yaml\n</code></pre> <p>The sample application consists of a half dozen deployments:  <code>productpage-v1</code>, <code>ratings-v1</code>, <code>details-v1</code>, and <code>reviews-v[1,2,3]</code>.</p> <p>Verify that all workloads are running, and have sidecars:</p> <pre><code>kubectl get pod\n</code></pre>"},{"location":"mesh-configurations/#configure-traffic-management","title":"Configure traffic management","text":"<p>Focus on two Istio custom resources, located in the <code>artifacts/mesh-config</code> folder:</p> <ul> <li> <p><code>dr-reviews.yaml</code>:  A DestinationRule that defines the subsets v1, v2, and v3 of the <code>reviews</code> service.</p> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: DestinationRule\nmetadata:\n  name: reviews\nspec:\n  host: reviews.default.svc.cluster.local\n  trafficPolicy:\n    loadBalancer:\n      simple: RANDOM\n  subsets:\n  - name: v1\n    labels:\n      version: v1\n  - name: v2\n    labels:\n      version: v2\n  - name: v3\n    labels:\n      version: v3\n</code></pre> </li> <li> <p><code>vs-reviews-v1.yaml</code>: A VirtualService that directs all traffic to the reviews service to the <code>v1</code> subset.</p> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: reviews\nspec:\n  hosts:\n  - reviews\n  http:\n  - route:\n    - destination:\n        host: reviews\n        subset: v1\n</code></pre> </li> </ul>"},{"location":"mesh-configurations/#validate-resources","title":"Validate resources","text":"<p>Istio provides a command to perform basic validation on Istio custom resources.</p> <p>Run the <code>istioctl validate</code> against each of these files:</p> <pre><code>istioctl validate -f artifacts/mesh-config/dr-reviews.yaml\n</code></pre> <p>And:</p> <pre><code>istioctl validate -f artifacts/mesh-config/vs-reviews-v1.yaml\n</code></pre> <p>It's handy to be able to validate all resources in the <code>mesh-config</code> folder:</p> <pre><code>istioctl validate -f artifacts/mesh-config\n</code></pre> <p>We learn that the resource syntax is valid. That does not necessarily imply correctness.</p> <p>The command cannot catch mistakes made in referencing label keys or values, or references to host names.  The <code>validate</code> command is more of a schema check.</p> <ul> <li> <p>Misspelling the keyword <code>subsets</code> (perhaps spelled in the singular) is something the <code>validate</code> command would catch.  Try it.</p> </li> <li> <p><code>validate</code> will also catch a wrong enum name.  For example, try to validate a version of <code>dr-reviews</code> where the loadBalancer value of RANDOM is instead spelled using lowercase.</p> </li> </ul> <p>Apply the DestinationRule and VirtualService to the cluster:</p> <pre><code>kubectl apply -f artifacts/mesh-config/dr-reviews.yaml\n</code></pre> <p>And:</p> <pre><code>kubectl apply -f artifacts/mesh-config/vs-reviews-v1.yaml\n</code></pre>"},{"location":"mesh-configurations/#ascertain-the-applied-configurations","title":"Ascertain the applied configurations","text":""},{"location":"mesh-configurations/#was-the-resource-created","title":"Was the resource created?","text":"<p>Did I get an error message when the resources were applied?</p> <p>The console output should confirm the resource was created:</p> <pre><code>destinationrule.networking.istio.io/reviews created\n</code></pre>"},{"location":"mesh-configurations/#is-the-resource-present","title":"Is the resource present?","text":"<p>Can I display the resource?</p> <pre><code>kubectl get destinationrule\n</code></pre>"},{"location":"mesh-configurations/#did-i-reference-the-right-namespace","title":"Did I reference the right namespace?","text":"<p>This is a common \"gotcha\", though in this example we are targeting the <code>default</code> namespace, and it's unlikely we got this wrong.</p>"},{"location":"mesh-configurations/#are-references-correct","title":"Are references correct?","text":"<p>The DestinationRule references a host.  Is the hostname spelled correctly?  If the host resides in a different namespace, the hostname should be fully qualified.</p> <p>The DestinationRule also references labels.  Are those labels correct?  They could be misspelled.</p> <p>We are also establishing a naming convention whereby each subset has the name v1, v2, and v3.</p> <p>The Virtual Service resource references a host, a destination host, and a subset.  Make sure they match the desired host name, target host name, and subset name.</p>"},{"location":"mesh-configurations/#istioctl-analyze","title":"<code>istioctl analyze</code>","text":"<p>The analyze command is more powerful than validate, and runs a catalog of analyses against resources applied to the cluster.</p> <p>Run the command against the default namespace:</p> <pre><code>istioctl analyze\n</code></pre> <p>There should be no issues.</p> <p>Edit the virtualservice and misspell the destination host field from <code>reviews</code> to, say, <code>reviewss</code>.</p> <p>Apply the updated VirtualService to the cluster.</p> <p>Re-run analyze.</p> <pre><code>Error [IST0101] (VirtualService default/reviews) Referenced host not found: \"reviewss\"\nError [IST0101] (VirtualService default/reviews) Referenced host+subset in destinationrule not found: \"reviewss+v1\"\nError: Analyzers found issues when analyzing namespace: default.\nSee https://istio.io/v1.22/docs/reference/config/analysis for more information about causes and resolutions.\n</code></pre> <p>To get an idea of the variety of checks that the <code>analyze</code> command performs, follow the above suggested link.</p> <p>In this instance, we are warned of the invalid references.</p> <p>We will revisit the <code>analyze</code> command in subsequent example scenarios.</p>"},{"location":"mesh-configurations/#istioctl-x-describe","title":"<code>istioctl x describe</code>","text":"<p>The describe command is yet another useful way to independently verify the configuration against the <code>reviews</code> service.</p> <pre><code>istioctl x describe svc reviews\n</code></pre> <p>Here is the output:</p> <pre><code>Service: reviews\n   Port: http 9080/HTTP targets pod port 9080\nDestinationRule: reviews for \"reviews.default.svc.cluster.local\"\n   Matching subsets: v1,v2,v3\n   Policies: load balancer\nVirtualService: reviews\n   1 HTTP route(s)\n</code></pre> <p>The above output confirms that multiple subsets are defined, that a VirtualService is associated with the service in question, and that the load balancer policy was altered.</p>"},{"location":"mesh-configurations/#do-we-have-the-desired-behavior","title":"Do we have the desired behavior?","text":"<p>Make repeated calls from <code>sleep</code> to the reviews service.  Do all requests go to v1?</p> <pre><code>kubectl exec deploy/sleep -- \\\n  curl -s reviews.default.svc.cluster.local:9080/reviews/123  | jq\n</code></pre> <p>Note the value of <code>podname</code> in each response.  It should have the prefix <code>reviews-v1</code>.</p> <pre><code>kubectl exec deploy/sleep -- \\\n  curl -s reviews.default.svc.cluster.local:9080/reviews/123  | jq .podname\n</code></pre> <p>That, ultimately, is the evidence we're looking for.</p>"},{"location":"mesh-configurations/#inspect-the-envoy-configuration","title":"Inspect the Envoy configuration","text":"<p>We can ask the question:  how is <code>sleep</code> configured to route requests to the <code>reviews</code> service?</p> <p>Ask Istio to show the Envoy configuration of the sidecar of the client, in this case <code>sleep</code>:</p> <pre><code>istioctl proxy-config routes deploy/sleep --name 9080 -o yaml\n</code></pre> <p>Here is the relevant section from the output:</p> <pre><code>- domains:\n  - reviews.default.svc.cluster.local\n  - reviews\n  - reviews.default.svc\n  - reviews.default\n  - 10.43.207.109\n  includeRequestAttemptCount: true\n  name: reviews.default.svc.cluster.local:9080\n  routes:\n  - decorator:\n      operation: reviews.default.svc.cluster.local:9080/*\n    match:\n      prefix: /\n    metadata:\n      filterMetadata:\n        istio:\n          config: /apis/networking.istio.io/v1alpha3/namespaces/default/virtual-service/reviews\n    route:\n      cluster: outbound|9080|v1|reviews.default.svc.cluster.local\n      maxGrpcTimeout: 0s\n      retryPolicy:\n        hostSelectionRetryMaxAttempts: \"5\"\n        numRetries: 2\n        retriableStatusCodes:\n        - 503\n        retryHostPredicate:\n        - name: envoy.retry_host_predicates.previous_hosts\n          typedConfig:\n            '@type': type.googleapis.com/envoy.extensions.retry.host.previous_hosts.v3.PreviousHostsPredicate\n        retryOn: connect-failure,refused-stream,unavailable,cancelled,retriable-status-codes\n      timeout: 0s\n</code></pre> <p>On line 19, the cluster reference is to the subset \"v1\".</p> <p>Understanding Envoy, how it's designed and configured, can go a long way to helping understand how to read these configuration files, and knowing where to look.</p>"},{"location":"mesh-configurations/#another-example","title":"Another example","text":"<p>How can we verify that the load balancer policy specified in the DestinationRule has been applied?</p> <p>That policy is associated with the destination service, which Envoy calls a \"cluster\".</p> <pre><code>istioctl proxy-config cluster deploy/sleep --fqdn reviews.default.svc.cluster.local -o yaml | grep lbPolicy\n</code></pre> <p>Is the value \"RANDOM\"?  It should match what we specified in the DestinationRule. There are four lbPolicies, one for each subset (v1, v2, v3, plus the top-level service).</p> <p>In contrast, try to look at the lbPolicy configured in the <code>sleep</code> sidecar for the <code>httpbin</code> service. What lbPolicy is used for calls to that destination?</p> <pre><code>istioctl proxy-config cluster deploy/sleep --fqdn httpbin.default.svc.cluster.local -o yaml | grep lbPolicy\n</code></pre> <p>We will revisit the <code>istioctl proxy-config</code> command in subsequent scenarios.</p> <p>The page titled Debugging Envoy and Istiod does a great job of explaining the <code>istioctl proxy-config</code> command in more detail.</p>"},{"location":"mesh-configurations/#using-the-correct-workload-selector","title":"Using the correct workload selector","text":"<p>Workload selectors feature in the configuration of many Istio resources.  They are used to specify which Envoy proxies to target, for different purposes.</p> <p>For example, an EnvoyFilter uses workload selectors to identify which proxies to program. An AuthorizationPolicy uses workload selectors to target the sidecars that control access to their workloads.</p> <p>Apply the following authorization policy, designed to allow only the <code>productpage</code> service to call the <code>reviews</code> service:</p> <pre><code>kubectl apply -f artifacts/mesh-config/authz-policy-reviews.yaml\n</code></pre> <pre><code>---\napiVersion: security.istio.io/v1beta1\nkind: AuthorizationPolicy\nmetadata:\n  name: allowed-reviews-clients\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: reviewss\n  action: ALLOW\n  rules:\n  - from:\n    - source:\n        principals: [\"cluster.local/ns/default/sa/bookinfo-productpage\"]\n</code></pre> <p>Verify that other workloads, say <code>sleep</code>, can now no longer call reviews:</p> <pre><code>kubectl exec deploy/sleep -- curl -s http://reviews:9080/reviews/123 | jq\n</code></pre> <p>Was the request denied?</p> <p>Run analyze:</p> <pre><code>istioctl analyze\n</code></pre> <pre><code>Warning [IST0127] (AuthorizationPolicy default/allowed-reviews-clients)\n  No matching workloads for this resource with the following labels: app=reviewss\n</code></pre> <p>Run the following command, which lists all authorization policies that applies to the reviews service:</p> <pre><code>istioctl x authz check svc/reviews\n</code></pre> <p>Do any rules show up in the output?</p> <p>It seems we have a typo in the workload selector, in the value specified for the label:  <code>reviewss</code> instead of <code>reviews</code>.</p> <p>Fix the authorization policy to use the correctly-spelled label.</p> <p>Now re-run analyze:</p> <pre><code>istioctl analyze\n</code></pre> <p>The validation issues should have gone away.</p> <p>Try the authz check again:</p> <pre><code>istioctl x authz check svc/reviews\n</code></pre> <p>There should now be a matching policy in the output.</p> <p>Finally, try to call reviews from sleep again:</p> <pre><code>kubectl exec deploy/sleep -- curl -s http://reviews:9080/reviews/123\n</code></pre> <p>This time the response should say <code>RBAC: access denied</code>.</p> <p>We can look at the configuration of the inbound listener on the sidecar associated with the <code>reviews-v1</code> workload:</p> <pre><code>istioctl proxy-config listener deploy/reviews-v1 --port 15006 -o yaml\n</code></pre> <p>The output is quite lengthy.  Here is the relevant portion, showing the Envoy RBAC filter present in the filter chain, and configured to allow <code>productpage</code> through:</p> <pre><code>- name: envoy.filters.http.rbac\n  typedConfig:\n    '@type': type.googleapis.com/envoy.extensions.filters.http.rbac.v3.RBAC\n    rules:\n      policies:\n        ns[default]-policy[allowed-reviews-clients]-rule[0]:\n          permissions:\n          - andRules:\n              rules:\n              - any: true\n          principals:\n          - andIds:\n              ids:\n              - orIds:\n                  ids:\n                  - authenticated:\n                      principalName:\n                        exact: spiffe://cluster.local/ns/default/sa/bookinfo-productpage\n</code></pre>"},{"location":"mesh-configurations/#ingress-gateways","title":"Ingress Gateways","text":"<p>Explore troubleshooting issues with ingress configuration.</p> <p>Apply the following resource to the cluster:</p> <pre><code>kubectl apply -f artifacts/mesh-config/bookinfo-gateway.yaml\n</code></pre> <pre><code>---\napiVersion: networking.istio.io/v1alpha3\nkind: Gateway\nmetadata:\n  name: bookinfo-gateway\nspec:\n  # The selector matches the ingress gateway pod labels.\n  # If you installed Istio using Helm following the standard documentation, this would be \"istio=ingress\"\n  selector:\n    istio: ingressgateway # use istio default controller\n  servers:\n  - port:\n      number: 8080\n      name: http\n      protocol: HTTP\n    hosts:\n    - \"*\"\n---\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: bookinfo\nspec:\n  hosts:\n  - \"*\"\n  gateways:\n  - bookinfo-gateway\n  http:\n  - match:\n    - uri:\n        exact: /productpage\n    - uri:\n        prefix: /static\n    - uri:\n        exact: /login\n    - uri:\n        exact: /logout\n    - uri:\n        prefix: /api/v1/products\n    route:\n    - destination:\n        host: productpage\n        port:\n          number: 9080\n</code></pre> <p><code>bookinfo-gateway.yaml</code> configures both a Gateway resource and a VirtualService to route incoming traffic to the <code>productpage</code> app.</p> <p>Assuming the local cluster deployed with k3d in setup, the ingress gateway is reachable on localhost, port 80:</p> <pre><code>export GATEWAY_IP=localhost\n</code></pre> <p>Then:</p> <pre><code>curl http://$GATEWAY_IP/productpage\n</code></pre> <p>There are many opportunities for misconfiguration:</p> <ol> <li> <p>The Gateway selector that selects the ingress gateway.</p> <p>If for example we get the selector wrong (edit the value to \"ingressgateways\"), running <code>istioctl analyze</code> will catch the invalid reference:</p> <pre><code>Error [IST0101] (Gateway default/bookinfo-gateway) Referenced selector not found: \"istio=ingressgateways\"\n</code></pre> </li> <li> <p>The VirtualService that references the gateway.</p> <p>Edit the gateway name to \"bookinfo-gateways\". Here too <code>istioctl analyze</code> will catch the invalid reference:</p> <pre><code>Error [IST0101] (VirtualService default/bookinfo) Referenced gateway not found: \"bookinfo-gateways\"\nWarning [IST0132] (VirtualService default/bookinfo) one or more host [*] defined in VirtualService default/bookinfo not found in Gateway default/bookinfo-gateways.\n</code></pre> </li> <li> <p>The routing rule with the destination workload name and port.</p> <p>Mistype the destination workload and watch <code>istioctl analyze</code> catch that too:</p> <pre><code>Error [IST0101] (VirtualService default/bookinfo) Referenced host not found: \"productpages\"\n</code></pre> </li> </ol>"},{"location":"obs-setup/","title":"Observability setup","text":"<p>The ability to troubleshoot depends on our ability to see what goes on in a system.</p> <p>The Envoy sidecars in Istio produce and expose metrics that can be ingested by Prometheus or some other metrics collection tool. Istio provides Grafana dashboards to support monitoring the health of both the data plane and the control plane.</p> <p>Distributed traces complement metrics, and help make sense of call graphs, and specifically highlight where time is spent. Envoys help here, though applications must be configured to propagate trace headers through the call graph. We will use the Zipkin dashboard to view distributed traces.</p> <p>Finally, Kiali is an open-source observability tool designed specifically for Istio, containing many features. We will focus on the call graph visualizations that Kiali generates.</p>"},{"location":"obs-setup/#deploy-observability-tools","title":"Deploy observability tools","text":"<p>Istio provides deployment manifests for each tool under the <code>samples/addons</code> folder in the Istio distribution. Each tool will be deployed to the <code>istio-system</code> namespace. In production, more work is required to properly deploy, and configure access to each tool.</p>"},{"location":"obs-setup/#deploy-prometheus","title":"Deploy Prometheus","text":"<pre><code>kubectl apply -f istio-1.22.0/samples/addons/prometheus.yaml\n</code></pre>"},{"location":"obs-setup/#deploy-grafana","title":"Deploy Grafana","text":"<pre><code>kubectl apply -f istio-1.22.0/samples/addons/grafana.yaml\n</code></pre>"},{"location":"obs-setup/#deploy-zipkin","title":"Deploy Zipkin","text":"<pre><code>kubectl apply -f istio-1.22.0/samples/addons/extras/zipkin.yaml\n</code></pre>"},{"location":"obs-setup/#deploy-kiali","title":"Deploy Kiali","text":"<pre><code>kubectl apply -f istio-1.22.0/samples/addons/kiali.yaml\n</code></pre> <p>Wait on each deployment to be ready.  Check on the workloads with:</p> <pre><code>kubectl get pod -n istio-system\n</code></pre>"},{"location":"obs-setup/#reconfigure-ingress-for-bookinfo","title":"Reconfigure ingress for <code>bookinfo</code>","text":"<p>Delete the ingress configuration to <code>httpbin</code>:</p> <pre><code>kubectl delete -f artifacts/data-plane/httpbin-gateway.yaml\n</code></pre> <p>In its place, configure ingress for <code>bookinfo</code>:</p> <pre><code>kubectl apply -f artifacts/mesh-config/bookinfo-gateway.yaml\n</code></pre> <p>Scale the productpage deployment back to one replica:</p> <pre><code>kubectl scale deploy productpage-v1 --replicas 1\n</code></pre> <p>Verify that ingress is functioning:</p> <pre><code>curl http://$GATEWAY_IP/productpage\n</code></pre>"},{"location":"obs-setup/#generate-a-load-against-bookinfo","title":"Generate a load against <code>bookinfo</code>","text":"<p>Many tools exist for sending traffic through a system; fortio is one.</p> <p>Here we will use a simple bash loop to send a slow, steady flow of requests through the ingress gateway:</p> <pre><code>bash -c \"while true; do curl --head http://$GATEWAY_IP/productpage; sleep 0.5; done\"\n</code></pre> <p>Leave this while loop running in its own terminal.</p>"},{"location":"obs-setup/#access-the-dashboards","title":"Access the dashboards","text":"<p>You will primarily work with Grafana, Zipkin and Kiali.</p> <p>Each dashboard can be accessed through the dashboard command.</p> <p>For example:</p> <pre><code>istioctl dashboard grafana\n</code></pre> <p>This should automatically cause a browser to open to the URL of the Grafana dashboard.</p> <p>Locate, and navigate to the Istio folder under \"Dashboards\" in the side navigation bar, containing a variety of pre-built monitoring dashboards.</p> <p>Go ahead and close the browser window and terminate the <code>istioctl dashboard</code> command (press Ctrl+C), and proceed to the next activity.</p>"},{"location":"setup/","title":"Setup","text":""},{"location":"setup/#kubernetes","title":"Kubernetes","text":"<p>Before we can begin exploring Istio debugging, we need a Kubernetes cluster.</p> <p>Provision a local Kubernetes cluster with k3d:</p> <pre><code>k3d cluster create my-k8s-cluster \\\n  --k3s-arg \"--disable=traefik@server:0\" \\\n  --port 80:80@loadbalancer \\\n  --port 443:443@loadbalancer\n</code></pre> <pre><code>kubectl config get-contexts\n</code></pre>"},{"location":"setup/#istio-distribution","title":"Istio distribution","text":"<p>Download a copy of the Istio distribution:</p> <pre><code>curl -L https://istio.io/downloadIstio | ISTIO_VERSION=1.21.2 sh -\n</code></pre> <p>Copy the file <code>istio-1.21.2/bin/istioctl</code> to your PATH.</p> <p>Verify that <code>istioctl</code> is in your PATH by running these commands:</p> <pre><code>istioctl help\n</code></pre> <p>And:</p> <pre><code>istioctl version\n</code></pre>"},{"location":"setup/#workshop-artifacts","title":"Workshop artifacts","text":"<p>Download all yaml artifacts referenced in all scenarios as a single .tgz file here.</p>"},{"location":"sidecar-injection/","title":"Sidecar Injection","text":""},{"location":"sidecar-injection/#objective","title":"Objective","text":"<p>To explore troubleshooting issues relating to sidecar injection in Istio.</p>"},{"location":"sidecar-injection/#introduction","title":"Introduction","text":"<p>The Istio documentation contains a guide on sidecar injection that explains the mechanism for configuring sidecar injection, and ways in which sidecar injection can be controlled.</p>"},{"location":"sidecar-injection/#main-lesson","title":"Main lesson","text":"<p>The most important thing to understand about sidecar injection is that it occurs at pod creation time.</p> <p>This means that if you configure a pod or a namespace for sidecar injection, it won't affect any pods that are already running.</p>"},{"location":"sidecar-injection/#example","title":"Example","text":"<p>Deploy the <code>httpbin</code> workload to the default namespace:</p> <pre><code>kubectl apply -f istio-1.21.2/samples/httpbin/httpbin.yaml\n</code></pre> <p>The simplest way to tell whether sidecar injection took place is to display the number containers running in the pod:</p> <pre><code>kubectl get pod\n</code></pre> <p>Note the READY column shows 1/1 (one out of one) containers.</p> <p>Label the <code>default</code> namespace for sidecar injection:</p> <pre><code>kubectl label ns default istio-injection=enabled\n</code></pre> <p>The above act is passive:  nothing will happen until a pod is created in the <code>default</code> namespace.</p> <p>Either:</p> <ul> <li> <p>Delete the pod and let the deployment create a new one in its place:</p> <pre><code>kubectl delete pod -l app=httpbin\n</code></pre> </li> <li> <p>Submit a command to the kube API server to restart the deployment:</p> <pre><code>kubectl rollout restart deploy httpbin\n</code></pre> </li> </ul> <p>List the pods in the <code>default</code> namespace once more:</p> <pre><code>kubectl get pod\n</code></pre> <p>The READY column now shows 2/2 containers.  We have a proxy.</p>"},{"location":"sidecar-injection/#ways-to-specify-sidecar-injection","title":"Ways to specify sidecar injection","text":""},{"location":"sidecar-injection/#automatic-at-the-namespace-level","title":"Automatic at the namespace level","text":"<p>Above, we used automatic sidecar injection.  This means that deployment manifests do not need to be modified.  Instead, a mutating admission webhook is given the chance to mutate the deployment specification before it is applied to the Kubernetes cluster.</p> <p>Verify that the mutating webhook <code>istio-sidecar-injection</code> exists on the cluster:</p> <pre><code>kubectl get mutatingwebhookconfigurations\n</code></pre> <p>The template used by the sidecar injector is encoded in the ConfigMap named <code>istio-sidecar-injector</code> in the <code>istio-system</code> namespace:</p> <pre><code>kubectl get cm -n istio-system istio-sidecar-injector -o yaml\n</code></pre> <p>Automatic sidecar injection at the namespace level is convenient, and the recommended method.</p>"},{"location":"sidecar-injection/#automatic-at-the-pod-level","title":"Automatic at the pod level","text":"<p>Istio provides a mechanism to control sidecar injection at the pod level, by labeling the pod with <code>sidecar.istio.io/inject</code> with the value \"true\".</p> <p>Instead of labeling the namespace, we could have just applied the label to the <code>httpbin</code> workload.</p>"},{"location":"sidecar-injection/#manual","title":"Manual","text":"<p>The Istio CLI provides the <code>kube-inject</code> command to render the template against a deployment manifest.</p> <p>For example:</p> <pre><code>istioctl kube-inject -f istio-1.21.2/samples/httpbin/httpbin.yaml &gt; injected.yaml\n</code></pre> <p>Inspect the generated file <code>injected.yaml</code> and confirm that the deployment resource now specifies two containers.</p> <p>Contrast the original Deployment resource with the transformed, injected one.</p> <p>Question</p> <p>What name does Istio use for the sidecar container?</p>"},{"location":"sidecar-injection/#troubleshooting","title":"Troubleshooting","text":"<p>The typical questions one should ask when troubleshooting sidecar injection include:</p> <ul> <li> <p>Was the namespace labeled for sidecar injection?</p> <p>This can be verified with:</p> <pre><code>kubectl get ns -Listio-injection\n</code></pre> </li> <li> <p>Was the deployment restarted after labeling the namespace?</p> <p>This is one of the more common reasons that the sidecar is not injected.</p> </li> </ul>"},{"location":"sidecar-injection/#how-many-proxies-does-istio-see","title":"How many proxies does Istio see?","text":"<p>Remember the <code>istioctl version</code> command?  Run it again. How many proxies are listed in the data plane? The original count was one. If the count still shows \"1\", it means that no additional proxies were deployed.</p>"},{"location":"sidecar-injection/#proxy-status","title":"Proxy status","text":"<p>Yet another diagnostic command is the proxy-status command</p> <pre><code>istioctl proxy-status\n</code></pre> <p>If <code>httpbin</code> is not listed in the output, it's an indication that Istio does not know about it, possibly because no sidecar was injected.</p> <p>We will revisit the <code>proxy-status</code> command later.</p>"},{"location":"sidecar-injection/#check-injection","title":"Check injection","text":"<p>The Istio CLI provides the check-inject command to check whether sidecar injection has taken place for a given workload:</p> <pre><code>istioctl x check-inject -l app=httpbin\n</code></pre> <p>Look for a green checkmark under the INJECTED column.</p>"},{"location":"sidecar-injection/#consequences-of-a-missing-sidecar","title":"Consequences of a missing sidecar","text":"<p>Fundamentally, a missing sidecar means that all traffic in and out of the pod will not be controlled by Istio.</p> <p>This implies that:</p> <ul> <li>Istio Custom Resources applied, targeting that workload, will have no effect, as there is no proxy to program.</li> <li>The workload will not be considered a part of the mesh.  No service discovery information will be communicated to peer workloads.</li> </ul> <p>In specific circumstances, it could mean that communication between the workload and other mesh workloads will not function.</p>"},{"location":"sidecar-injection/#example-scenario","title":"Example scenario","text":"<p>Remove the sidecar injection label from the namespace:</p> <pre><code>kubectl label ns default istio-injection-\n</code></pre> <p>Deploy the <code>sleep</code> sample, a convenience client from which to <code>curl</code> other workloads:</p> <pre><code>kubectl apply -f istio-1.21.2/samples/sleep/sleep.yaml\n</code></pre> <p>Note that the <code>sleep</code> worklaod has no sidecar.</p> <p>Yet, we can still call <code>httpbin</code>:</p> <pre><code>kubectl exec deploy/sleep -- curl httpbin:8000/get\n</code></pre> <p>However, if our mesh was configured with strict mutual TLS Peer Authentication:</p> <pre><code>kubectl apply -f artifacts/injection/mtls-strict.yaml\n</code></pre> <p>An attempt to call <code>httpbin</code> once more will fail:</p> <pre><code>kubectl exec deploy/sleep -- curl httpbin:8000/get\n</code></pre> <p>The command fails with a <code>Connection reset by peer</code> error, because there is no forward proxy on <code>sleep</code> to upgrade the connection to mutual TLS, which is now a requirement.</p>"},{"location":"sidecar-injection/#remedy-the-situation","title":"Remedy the situation","text":"<p>Run:</p> <pre><code>istioctl proxy-status\n</code></pre> <p>Note that <code>sleep</code> is not on the list.</p> <p>Label the namespace once more, and restart the <code>sleep</code> deployment:</p> <pre><code>kubectl label ns default istio-injection=enabled &amp;&amp; kubectl rollout restart deploy sleep\n</code></pre> <p>Re-run:</p> <pre><code>istioctl proxy-status\n</code></pre> <p>And confirm that <code>sleep</code> is now listed as a proxy.</p> <p>Finally, call <code>httpbin</code> once more:</p> <pre><code>kubectl exec deploy/sleep -- curl httpbin:8000/get\n</code></pre> <p>This time it succeeds.</p>"},{"location":"sidecar-injection/#sidecar-injection-problems","title":"Sidecar injection problems","text":"<p>The Istio docs have a page titled sidecar injection problems that catalogs sidecar injection errors and their remedies.</p>"},{"location":"sidecar-injection/#communication-between-proxies-and-istiod","title":"Communication between proxies and <code>istiod</code>","text":"<p>It is important to realize that <code>istiod</code> has a constant line of communication with all of the Envoy's:  both gateways and sidecars.</p> <p>When we run <code>istioctl proxy-status</code> we get insight into all the proxies that <code>istiod</code> controls, and whether the latest configurations have been sent and synced with each proxy.</p> <p>A common issue is forgetting to upgrade the sidecars after an Istio upgrade.</p> <p>To illustrate the issue, upgrade Istio in place.</p>"},{"location":"sidecar-injection/#upgrading-istio-and-dangling-sidecars","title":"Upgrading Istio and \"dangling\" sidecars","text":"<p>Download a newer version of Istio, version 1.22.0:</p> <pre><code>curl -L https://istio.io/downloadIstio | ISTIO_VERSION=1.22.0 sh -\n</code></pre> <p>Replace the <code>istioctl</code> CLI in your PATH with the one from the new distribution.</p> <p>Verify that the Istio CLI version is now version 1.22.0:</p> <pre><code>client version: 1.22.0\ncontrol plane version: 1.21.2\ndata plane version: 1.21.2 (3 proxies)\n</code></pre> <p>Upgrade Istio in-place:</p> <pre><code>istioctl upgrade -f artifacts/install/trace-config.yaml\n</code></pre> <p>Re-run <code>istioctl version</code> and note the output:</p> <pre><code>client version: 1.22.0\ncontrol plane version: 1.22.0\ndata plane version: 1.21.2 (2 proxies), 1.22.0 (1 proxies)\n</code></pre> <p>What is going on here?</p> <ol> <li>The Istio CLI was upgraded</li> <li>The control plane was also upgraded</li> <li>The ingress gateway component, running Envoy, was also upgraded</li> <li>However, note that two proxies are still associated with the older version of Istio</li> </ol> <p>Get more information with:</p> <pre><code>istioctl proxy-status\n</code></pre> <p>We can see that <code>httpbin</code> and <code>sleep</code> were left alone. They still have a sidecar, but they are not associated with the new control plane. It's a sort of \"orphaned\" sidecar in that the new, updated Istio is not communicating with it.</p> <p>After an upgrade, it's important to be aware that the sidecars (the data plane) are not updated automatically.</p> <p>To update <code>httpbin</code>'s and <code>sleep</code>'s sidecars, restart the deployments:</p> <pre><code>for name in `kubectl get deploy -oname`; do\n  kubectl rollout restart $name;\ndone\n</code></pre> <p>Re-run <code>istioctl proxy-status</code> and verify that the <code>httpbin</code> and <code>sleep</code> sidecars are now associated with Istio version 1.22.0.</p>"}]}