{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":"<p>The materials on this portal help you explore ways of debugging and troubleshooting Istio.</p> <p>You will learn about different commands and tools for ascertaining the state of your mesh and otherwise troubleshoot a particular issue.</p>"},{"location":"controlplane-health/","title":"Control plane Health","text":""},{"location":"controlplane-health/#todo","title":"TODO","text":"<ul> <li>istiod logs</li> <li>istioctl admin log</li> <li>are there errors in the logs?</li> <li>metrics:<ul> <li>pilot error rate</li> <li>istio validation error rate</li> <li>sidecar injection error rate</li> </ul> </li> <li> <p>monitor the control plane - specific metrics to look for</p> <ul> <li>the istio control plane grafana dashboard</li> </ul> </li> <li> <p>what governs how much work istio has to do?</p> <ul> <li>number of running workloads / services</li> <li>frequency of deployment changes (adding, removing, updating)</li> <li>frequency of mesh configuration changes</li> <li>service discoverability scope (use sidecar resources)</li> </ul> </li> </ul>"},{"location":"data-plane/","title":"The data plane","text":"<p>In the previous exploration we looked at issues relating to misconfiguration.</p> <p>In this exploration, we investigate issues with the data plane:  everything is configured correctly, but some traffic flow isn't functioning, and we need to find out why.</p> <p>Ingress is configured for the <code>bookinfo</code> application, routing requests to the <code>productpage</code> destination.</p> <p>Assuming the local cluster deployed with k3d in setup, the ingress gateway is reachable on localhost, port 80:</p> <pre><code>export GATEWAY_IP=localhost\n</code></pre>"},{"location":"data-plane/#no-healthy-upstream-uh","title":"No Healthy Upstream (UH)","text":"<p>What if for some reason the backing workload is not accessible?</p> <p>Simulate this situation by scaling the <code>productpage-v1</code> deployment to zero replicas:</p> <pre><code>kubectl scale deploy productpage-v1 --replicas 0\n</code></pre> <p>In one terminal, tail the logs of the ingress gateway:</p> <pre><code>kubectl logs --follow -n istio-system -l istio=ingressgateway\n</code></pre> <p>In another terminal, send a request in through the ingress gateway:</p> <pre><code>curl http://$GATEWAY_IP/productpage\n</code></pre> <p>In the logs you should see the following line:</p> <pre><code>\"GET /productpage HTTP/1.1\" 503 UH no_healthy_upstream - \"-\" 0 19 0 - \"10.42.0.1\" \"curl/8.7.1\" \"c4c58af1-2066-4c45-affb-d1345d32fc66\" \"localhost\" \"-\" outbound|9080||productpage.default.svc.cluster.local - 10.42.0.7:8080 10.42.0.1:60667 - -\n</code></pre> <p>Note the UH response flag:  No Healthy Upstream.</p> <p>These response flags clearly communicate to the operator the reason for which the request did not succeed.</p>"},{"location":"data-plane/#no-route-found-nr","title":"No Route Found (NR)","text":"<p>As another example, make a request to a route that does not match any routing rules in the virtual service:</p> <pre><code>curl http://$GATEWAY_IP/productpages\n</code></pre> <p>The log entry responds with a 404 \"NR\", for \"No Route Found\":</p> <pre><code>\"GET /productpages HTTP/1.1\" 404 NR route_not_found - \"-\" 0 0 0 - \"10.42.0.1\" \"curl/8.7.1\" \"2606aaa9-8c5c-4987-9ba7-86b89f901d34\" \"localhost\" \"-\" - - 10.42.0.7:8080 10.42.0.1:13819 - -\n</code></pre>"},{"location":"data-plane/#upstreamretrylimitexceeded-urx","title":"UpstreamRetryLimitExceeded (URX)","text":"<p>Delete the <code>bookinfo</code> Gateway and VirtualService resources:</p> <pre><code>kubectl delete -f artifacts/mesh-config/bookinfo-gateway.yaml\n</code></pre> <p>In its place, configure ingress for the <code>httpbin</code> workload:</p> <pre><code>kubectl apply -f artifacts/data-plane/httpbin-gateway.yaml\n</code></pre> <pre><code>---\napiVersion: networking.istio.io/v1alpha3\nkind: Gateway\nmetadata:\n  name: httpbin-gateway\nspec:\n  selector:\n    istio: ingressgateway\n  servers:\n  - port:\n      number: 80\n      name: http\n      protocol: HTTP\n    hosts:\n    - \"*\"\n---\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: httpbin\nspec:\n  hosts:\n  - \"*\"\n  gateways:\n  - httpbin-gateway\n  http:\n  - route:\n    - destination:\n        host: httpbin\n        port:\n          number: 8000\n    retries:\n      attempts: 3\n      retryOn: gateway-error,connect-failure,refused-stream\n</code></pre> <p>The VirtualService is configured with three retry attempts in the event of a 503 response.</p> <p>Call the <code>httpbin</code> endpoint that returns a 503:</p> <pre><code>curl http://$GATEWAY_IP/status/503\n</code></pre> <p>The Envoy gateway logs will show the response flag URX:  UpstreamRetryLimitExceeded:</p> <pre><code>\"GET /status/503 HTTP/1.1\" 503 URX via_upstream - \"-\" 0 0 120 119 \"10.42.0.1\" \"curl/8.7.1\" \"dcb3b100-e296-4031-8f45-1234d20b0f20\" \"localhost\" \"10.42.0.9:8080\" outbound|8000||httpbin.default.svc.cluster.local 10.42.0.7:38902 10.42.0.7:8080 10.42.0.1:51761 - -\n</code></pre> <p>That is, the gateway got a 503, retried the request up to three times, and then gave up.</p> <p>Envoy's response flags provide insight into why a request to a target destination workload might have failed.</p>"},{"location":"data-plane/#sidecar-logs","title":"Sidecar logs","text":"<p>We are not restricted to inspecting the logs of the ingress gateway.  We can also check the logs of the Envoy sidecars.</p> <p>Tail the logs for the sidecar of the <code>httpbin</code> destination workload:</p> <pre><code>kubectl logs --follow deploy/httpbin -c istio-proxy\n</code></pre> <p>Repeat the call to the <code>httpbin</code> \"503\" endpoint:</p> <pre><code>curl http://$GATEWAY_IP/status/503\n</code></pre> <p>You will see evidence of four inbound requests received by the sidecar, i.e. three retry attempts.</p>"},{"location":"data-plane/#log-levels","title":"Log levels","text":"<p>The log level for any Envoy proxy can be either displayed or configured with the proxy-config log command.</p> <p>Envoy has many loggers.  The log level for each logger can be configured independently.</p> <p>For example, let us target the Istio ingress gateway deployment.</p> <p>To view the log levels for each logger, run:</p> <pre><code>istioctl proxy-config log -n istio-system deploy/istio-ingressgateway\n</code></pre> <p>The log levels are: <code>trace</code>, <code>debug</code>, <code>info</code>, <code>warning</code>, <code>error</code>, <code>critical</code>, and <code>off</code>.</p> <p>To set the log level for, say the wasm logger, to <code>info</code>:</p> <pre><code>istioctl proxy-config log -n istio-system deploy/istio-ingressgateway --level wasm:info\n</code></pre> <p>This can be useful for debugging wasm extensions.</p> <p>The output displays the updated logging levels for every logger for that Envoy instance.</p> <p>Log levels can be reset for all loggers with the <code>--reset</code> flag:</p> <pre><code>istioctl proxy-config log -n istio-system deploy/istio-ingressgateway --reset\n</code></pre>"},{"location":"dataplane-health/","title":"Data plane Health","text":""},{"location":"dataplane-health/#todo","title":"TODO","text":"<ul> <li>call graph visualizations (kiali)</li> <li>distributed trace visualizations to uncover latency issues</li> <li>istio service &amp; workload grafana dashboards</li> <li>data plane metrics:<ul> <li>high 4xx error rate</li> <li>high 5xx error rate</li> <li>high request latency</li> <li>latency 99 percentile</li> </ul> </li> </ul>"},{"location":"install/","title":"Installation","text":""},{"location":"install/#objective","title":"Objective","text":"<p>To explore troubleshooting and diagnostic commands associated with the installation of Istio.</p>"},{"location":"install/#precheck","title":"Precheck","text":"<p>The precheck command helps ascertain that Istio can be installed or upgraded on the current Kubernetes cluster:</p> <pre><code>istioctl x precheck\n</code></pre>"},{"location":"install/#install-istio","title":"Install Istio","text":"<p>Istio is often installed with the <code>istioctl</code> CLI in sandbox environments.</p> <p><code>helm</code> is typically the method preferred for QA, staging, and production environments.</p> <p>Use the Istio CLI to install Istio with the default configuration profile, which deploys <code>istiod</code> and the Istio ingress gateway component:</p> <pre><code>istioctl install -f artifacts/install/trace-config.yaml\n</code></pre> <p>Above, we also reference an installation configuration that configures distributed tracing.</p> <pre><code>---\napiVersion: install.istio.io/v1alpha1\nkind: IstioOperator\nspec:\n  profile: default\n  meshConfig:\n    enableTracing: true\n    defaultConfig:\n      tracing:\n        sampling: 100.0\n    extensionProviders:\n    - name: zipkin\n      zipkin:\n        service: zipkin.istio-system.svc.cluster.local\n        port: 9411\n</code></pre>"},{"location":"install/#is-istio-installed-properly","title":"Is istio installed properly?","text":"<p>Given an environment with Istio installed, we can verify the installation with the verify-install command:</p> <pre><code>istioctl verify-install\n</code></pre>"},{"location":"install/#what-version-of-istio-am-i-running","title":"What version of Istio am I running?","text":"<p>The version command provides version information for the CLI, the control plane, and the data plane (proxies):</p> <pre><code>istioctl version\n</code></pre> <p>Question</p> <p>Part of the output from <code>istioctl version</code> is:</p> <pre><code>data plane version: 1.21.2 (1 proxies)\n</code></pre> <p>Can you explain what this means?  What proxies are being referred to?</p>"},{"location":"install/#access-logging","title":"Access Logging","text":"<p>When using the <code>default</code> configuration profile, Envoy sidecars and gateways are not default-configured with access logging to standard output.</p> <p>We can enable Envoy Access logging using Istio's Telemetry API.</p> <p>Apply the following resource to your Kubernetes cluster:</p> <pre><code>kubectl apply -f artifacts/install/telemetry.yaml\n</code></pre> <pre><code>---\napiVersion: telemetry.istio.io/v1alpha1\nkind: Telemetry\nmetadata:\n  name: mesh-default\n  namespace: istio-system\nspec:\n  accessLogging:\n  - providers:\n    - name: envoy\n  tracing:\n  - providers:\n    - name: zipkin\n</code></pre> <p>Note that we also set the tracing provider to zipkin.  More on that later.</p>"},{"location":"mesh-configurations/","title":"Mesh Configurations","text":"<p>This section explores troubleshooting mesh configurations, and what tools are at your disposal to validate and diagnose configuration-related issues.</p>"},{"location":"mesh-configurations/#deploy-the-bookinfo-sample-application","title":"Deploy the <code>bookinfo</code> sample application","text":"<p>Take a moment to familiarize yourself with the Istio sample application <code>bookinfo</code>.</p> <p>Verify that the <code>default</code> namespace is labeled for sidecar injection:</p> <pre><code>kubectl get ns -Listio-injection\n</code></pre> <p>If it isn't labeled, the first step is to label it:</p> <pre><code>kubectl label ns default istio-injection=enabled\n</code></pre> <p>Deploy the bookinfo sample application to the <code>default</code> namespace:</p> <pre><code>kubectl apply -f istio-1.22.0/samples/bookinfo/platform/kube/bookinfo.yaml\n</code></pre> <p>The sample application consists of a half dozen deployments:  <code>productpage-v1</code>, <code>ratings-v1</code>, <code>details-v1</code>, and <code>reviews-v[1,2,3]</code>.</p> <p>Verify that all workloads are running, and have sidecars:</p> <pre><code>kubectl get pod\n</code></pre>"},{"location":"mesh-configurations/#configure-traffic-management","title":"Configure traffic management","text":"<p>Focus on two Istio custom resources, located in the <code>artifacts/mesh-config</code> folder:</p> <ul> <li> <p><code>dr-reviews.yaml</code>:  A DestinationRule that defines the subsets v1, v2, and v3 of the <code>reviews</code> service.</p> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: DestinationRule\nmetadata:\n  name: reviews\nspec:\n  host: reviews.default.svc.cluster.local\n  trafficPolicy:\n    loadBalancer:\n      simple: RANDOM\n  subsets:\n  - name: v1\n    labels:\n      version: v1\n  - name: v2\n    labels:\n      version: v2\n  - name: v3\n    labels:\n      version: v3\n</code></pre> </li> <li> <p><code>vs-reviews-v1.yaml</code>: A VirtualService that directs all traffic to the reviews service to the <code>v1</code> subset.</p> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: reviews\nspec:\n  hosts:\n  - reviews\n  http:\n  - route:\n    - destination:\n        host: reviews\n        subset: v1\n</code></pre> </li> </ul>"},{"location":"mesh-configurations/#validate-resources","title":"Validate resources","text":"<p>Istio provides a command to perform basic validation on Istio custom resources.</p> <p>Run the <code>istioctl validate</code> against each of these files:</p> <pre><code>istioctl validate -f artifacts/mesh-config/dr-reviews.yaml\n</code></pre> <pre><code>istioctl validate -f artifacts/mesh-config/vs-reviews-v1.yaml\n</code></pre> <p>We learn that the resource syntax is valid. That does not necessarily imply correctness.</p> <p>The command cannot catch mistakes made in referencing label keys or values, or references to host names.  The <code>validate</code> command is more of a schema check.</p> <ul> <li> <p>Misspelling the keyword <code>subsets</code> (perhaps spelled in the singular) is something the <code>validate</code> command would catch.  Try it.</p> </li> <li> <p><code>validate</code> will also catch a wrong enum name.  For example, try to validate a version of <code>dr-reviews</code> where the loadBalancer value of RANDOM is instead spelled using lowercase.</p> </li> </ul> <p>Apply the DestinationRule and VirtualService to the cluster:</p> <pre><code>kubectl apply -f artifacts/mesh-config/dr-reviews.yaml\n</code></pre> <p>And:</p> <pre><code>kubectl apply -f artifacts/mesh-config/vs-reviews-v1.yaml\n</code></pre>"},{"location":"mesh-configurations/#ascertain-the-applied-configurations","title":"Ascertain the applied configurations","text":""},{"location":"mesh-configurations/#was-the-resource-created","title":"Was the resource created?","text":"<p>Did I get an error message when the resources were applied?</p> <p>The console output should confirm the resource was created:</p> <pre><code>destinationrule.networking.istio.io/reviews created\n</code></pre>"},{"location":"mesh-configurations/#is-the-resource-present","title":"Is the resource present?","text":"<p>Can I display the resource?</p> <pre><code>kubectl get destinationrule\n</code></pre>"},{"location":"mesh-configurations/#did-i-reference-the-right-namespace","title":"Did I reference the right namespace?","text":"<p>This is a common \"gotcha\", though in this example we are targeting the <code>default</code> namespace, and it's unlikely we got this wrong.</p>"},{"location":"mesh-configurations/#are-references-correct","title":"Are references correct?","text":"<p>The DestinationRule references a host.  Is the hostname spelled correctly?  If the host resides in a different namespace, the hostname should be fully qualified.</p> <p>The DestinationRule also references labels.  Are those labels correct?  They could be misspelled.</p> <p>We are also establishing a naming convention whereby each subset has the name v1, v2, and v3.</p> <p>The Virtual Service resource references a host, a destination host, and a subset.  Make sure they match the desired host name, target host name, and subset name.</p>"},{"location":"mesh-configurations/#istioctl-analyze","title":"<code>istioctl analyze</code>","text":"<p>The analyze command is more powerful than validate, and runs a catalog of analyses against resources applied to the cluster.</p> <p>Run the command against the default namespace:</p> <pre><code>istioctl analyze\n</code></pre> <p>There should be no issues.</p> <p>Edit the virtualservice and misspell the host field from <code>reviews</code> to, say, <code>reviewss</code>.</p> <p>Apply the updated virtualservice to the cluster.</p> <p>Re-run analyze.</p> <pre><code>Error [IST0101] (VirtualService default/reviews) Referenced host not found: \"reviewss\"\nError [IST0101] (VirtualService default/reviews) Referenced host+subset in destinationrule not found: \"reviewss+v1\"\nError: Analyzers found issues when analyzing namespace: default.\nSee https://istio.io/v1.22/docs/reference/config/analysis for more information about causes and resolutions.\n</code></pre> <p>To get an idea of the variety of checks that the analyze command performs, follow the above suggested link.</p> <p>In this instance we're warned of the invalid references.</p> <p>We will revisit the <code>analyze</code> command in subsequent example scenarios.</p>"},{"location":"mesh-configurations/#istioctl-x-describe","title":"<code>istioctl x describe</code>","text":"<p>The describe command is yet another useful way to independently verify the configuration against the <code>reviews</code> service.</p> <pre><code>istioctl x describe svc reviews\n</code></pre> <p>Here is the output:</p> <pre><code>Service: reviews\n   Port: http 9080/HTTP targets pod port 9080\nDestinationRule: reviews for \"reviews.default.svc.cluster.local\"\n   Matching subsets: v1,v2,v3\n   Policies: load balancer\nVirtualService: reviews\n   1 HTTP route(s)\n</code></pre> <p>The above output confirms that multiple subsets are defined, that a VirtualService is associated with the service in question, and that the load balancer policy was altered.</p>"},{"location":"mesh-configurations/#do-we-have-the-desired-behavior","title":"Do we have the desired behavior?","text":"<p>Make repeated calls from <code>sleep</code> to the reviews service.  Do all requests go to v1?</p> <pre><code>kubectl exec deploy/sleep -- \\\n  curl -s reviews.default.svc.cluster.local:9080/reviews/123  | jq\n</code></pre> <p>Note the value of <code>podname</code> in each response.  It should have the prefix <code>reviews-v1</code>.</p> <p>That, ultimately, is the evidence we're looking for.</p>"},{"location":"mesh-configurations/#inspect-the-envoy-configuration","title":"Inspect the Envoy configuration","text":"<p>We can ask the question:  how is <code>sleep</code> configured to route requests to the <code>reviews</code> service?</p> <p>We can ask Istio to show us the Envoy configuration of the sidecar of the client, in this case <code>sleep</code>:</p> <pre><code>istioctl proxy-config routes deploy/sleep --name 9080 -o yaml\n</code></pre> <p>Here is the relevant section from the output:</p> <pre><code>- domains:\n  - reviews.default.svc.cluster.local\n  - reviews\n  - reviews.default.svc\n  - reviews.default\n  - 10.43.207.109\n  includeRequestAttemptCount: true\n  name: reviews.default.svc.cluster.local:9080\n  routes:\n  - decorator:\n      operation: reviews.default.svc.cluster.local:9080/*\n    match:\n      prefix: /\n    metadata:\n      filterMetadata:\n        istio:\n          config: /apis/networking.istio.io/v1alpha3/namespaces/default/virtual-service/reviews\n    route:\n      cluster: outbound|9080|v1|reviews.default.svc.cluster.local\n      maxGrpcTimeout: 0s\n      retryPolicy:\n        hostSelectionRetryMaxAttempts: \"5\"\n        numRetries: 2\n        retriableStatusCodes:\n        - 503\n        retryHostPredicate:\n        - name: envoy.retry_host_predicates.previous_hosts\n          typedConfig:\n            '@type': type.googleapis.com/envoy.extensions.retry.host.previous_hosts.v3.PreviousHostsPredicate\n        retryOn: connect-failure,refused-stream,unavailable,cancelled,retriable-status-codes\n      timeout: 0s\n</code></pre> <p>On line 19, the cluster reference is to the subset \"v1\".</p> <p>Understanding Envoy, how it's designed and configured, can go a long way to helping understand how to read these configuration files, and knowing where to look.</p>"},{"location":"mesh-configurations/#another-example","title":"Another example","text":"<p>How can we verify that the load balancer policy specified in the DestinationRule has been applied?</p> <p>That policy is associated with the destination service, which Envoy calls a \"cluster\".</p> <pre><code>istioctl proxy-config cluster deploy/sleep --fqdn reviews.default.svc.cluster.local -o yaml | grep lbPolicy\n</code></pre> <p>Is the value \"RANDOM\"?  It should match what we specified in the DestinationRule. There are four lbPolicies, one for each subset (v1, v2, v3, plus the top-level service).</p> <p>In contrast, try to look at the lbPolicy configured in the <code>sleep</code> sidecar for the <code>httpbin</code> service. What lbPolicy is used for calls to that destination?</p> <p>We will revisit the <code>istioctl proxy-config</code> command in subsequent scenarios.</p> <p>The page titled Debugging Envoy and Istiod does a great job of explaining the <code>istioctl proxy-config</code> command in more detail.</p>"},{"location":"mesh-configurations/#using-the-correct-workload-selector","title":"Using the correct workload selector","text":"<p>Workload selectors feature in the configuration of many Istio resources.  They are used to specify which Envoy proxies to target, for different purposes.</p> <p>For example, an EnvoyFilter uses workload selectors to identify which proxies to program. An AuthorizationPolicy uses workload selectors to target the sidecars that control access to their workloads.</p> <p>Apply the following authorization policy, designed to allow only the <code>productpage</code> service to call the <code>reviews</code> service:</p> <pre><code>kubectl apply -f artifacts/mesh-config/authz-policy-reviews.yaml\n</code></pre> <pre><code>---\napiVersion: security.istio.io/v1beta1\nkind: AuthorizationPolicy\nmetadata:\n  name: allowed-reviews-clients\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: reviewss\n  action: ALLOW\n  rules:\n  - from:\n    - source:\n        principals: [\"cluster.local/ns/default/sa/bookinfo-productpage\"]\n</code></pre> <p>Verify that other workloads, say <code>sleep</code>, can now no longer call reviews:</p> <pre><code>kubectl exec deploy/sleep -- curl -s http://reviews:9080/reviews/123\n</code></pre> <p>Was the request denied?</p> <p>Run analyze:</p> <pre><code>istioctl analyze\n</code></pre> <pre><code>Warning [IST0127] (AuthorizationPolicy default/allowed-reviews-clients) No matching workloads for this resource with the following labels: app=reviewss\n</code></pre> <p>Run the following command, which lists all authorization policies that applies to the reviews service:</p> <pre><code>istioctl x authz check svc/reviews\n</code></pre> <p>Do any rules show up in the output?</p> <p>It seems we have a typo in the workload selector, in the value specified for the label:  <code>reviewss</code> instead of <code>reviews</code>.</p> <p>Fix the authorization policy to use the correctly-spelled label.</p> <p>Now re-run analyze:</p> <pre><code>istioctl analyze\n</code></pre> <p>The validation issues should have gone away.</p> <p>Try the authz check again:</p> <pre><code>istioctl x authz check svc/reviews\n</code></pre> <p>There should now be a matching policy in the output.</p> <p>Finally, try to call reviews from sleep again:</p> <pre><code>kubectl exec deploy/sleep -- curl -s http://reviews:9080/reviews/123\n</code></pre> <p>This time the response should say <code>RBAC: access denied</code>.</p> <p>We can look at the configuration of the inbound listener on the sidecar associated with the <code>reviews-v1</code> workload:</p> <pre><code>istioctl proxy-config listener deploy/reviews-v1 --port 15006 -o yaml\n</code></pre> <p>The output is quite lengthy.  Here is the relevant portion, showing the Envoy RBAC filter present in the filter chain, and configured to allow <code>productpage</code> through:</p> <pre><code>- name: envoy.filters.http.rbac\n  typedConfig:\n    '@type': type.googleapis.com/envoy.extensions.filters.http.rbac.v3.RBAC\n    rules:\n      policies:\n        ns[default]-policy[allowed-reviews-clients]-rule[0]:\n          permissions:\n          - andRules:\n              rules:\n              - any: true\n          principals:\n          - andIds:\n              ids:\n              - orIds:\n                  ids:\n                  - authenticated:\n                      principalName:\n                        exact: spiffe://cluster.local/ns/default/sa/bookinfo-productpage\n</code></pre>"},{"location":"mesh-configurations/#ingress-gateways","title":"Ingress Gateways","text":"<p>Explore troubleshooting issues with ingress configuration.</p> <p>Apply the following resource to the cluster:</p> <pre><code>kubectl apply -f artifacts/mesh-config/bookinfo-gateway.yaml\n</code></pre> <pre><code>---\napiVersion: networking.istio.io/v1alpha3\nkind: Gateway\nmetadata:\n  name: bookinfo-gateway\nspec:\n  # The selector matches the ingress gateway pod labels.\n  # If you installed Istio using Helm following the standard documentation, this would be \"istio=ingress\"\n  selector:\n    istio: ingressgateway # use istio default controller\n  servers:\n  - port:\n      number: 8080\n      name: http\n      protocol: HTTP\n    hosts:\n    - \"*\"\n---\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: bookinfo\nspec:\n  hosts:\n  - \"*\"\n  gateways:\n  - bookinfo-gateway\n  http:\n  - match:\n    - uri:\n        exact: /productpage\n    - uri:\n        prefix: /static\n    - uri:\n        exact: /login\n    - uri:\n        exact: /logout\n    - uri:\n        prefix: /api/v1/products\n    route:\n    - destination:\n        host: productpage\n        port:\n          number: 9080\n</code></pre> <p><code>bookinfo-gateway.yaml</code> configures both a Gateway resource and a VirtualService to route incoming traffic to the <code>productpage</code> app.</p> <p>Assuming the local cluster deployed with k3d in setup, the ingress gateway is reachable on localhost, port 80:</p> <pre><code>export GATEWAY_IP=localhost\n</code></pre> <p>Then:</p> <pre><code>curl http://$GATEWAY_IP/productpage\n</code></pre> <p>There are many opportunities for misconfiguration:</p> <ol> <li> <p>The Gateway selector that selects the ingress gateway.</p> <p>If for example we get the selector wrong (edit the value to \"ingressgateways\"), running <code>istioctl analyze</code> will catch the invalid reference:</p> <pre><code>Error [IST0101] (Gateway default/bookinfo-gateway) Referenced selector not found: \"istio=ingressgateways\"\n</code></pre> </li> <li> <p>The VirtualService that references the gateway.</p> <p>Edit the gateway name to \"bookinfo-gateways\". Here too <code>istioctl analyze</code> will catch the invalid reference:</p> <pre><code>Error [IST0101] (VirtualService default/bookinfo) Referenced gateway not found: \"bookinfo-gateways\"\nWarning [IST0132] (VirtualService default/bookinfo) one or more host [*] defined in VirtualService default/bookinfo not found in Gateway default/bookinfo-gateways.\n</code></pre> </li> <li> <p>The routing rule with the destination workload name and port.</p> <p>Mistype the destination workload and watch <code>istioctl analyze</code> catch that too:</p> <pre><code>Error [IST0101] (VirtualService default/bookinfo) Referenced host not found: \"productpages\"\n</code></pre> </li> </ol>"},{"location":"obs-setup/","title":"Observability setup","text":"<p>The ability to troubleshoot depends on our ability to see what goes on in a system.</p> <p>The Envoy sidecars in Istio produce and expose metrics that can be ingested by Prometheus or some other metrics collection tool. Istio provides Grafana dashboards to support monitoring the health of both the data plane and the control plane.</p> <p>Distributed traces complement metrics, and help make sense of call graphs, and specifically highlight where time is spent. Envoys help here, though applications must be configured to propagate trace headers through the call graph. We will use the Zipkin dashboard to view distributed traces.</p> <p>Finally, Kiali is an open-source observability tool designed specifically for Istio, containing many features. We will focus on the call graph visualizations that Kiali generates.</p>"},{"location":"obs-setup/#deploy-observability-tools","title":"Deploy observability tools","text":"<p>Istio provides deployment manifests for each tool under the <code>samples/addons</code> folder in the Istio distribution. Each tool will be deployed to the <code>istio-system</code> namespace. In production, more work is required to properly deploy, and configure access to each tool.</p>"},{"location":"obs-setup/#deploy-prometheus","title":"Deploy Prometheus","text":"<pre><code>kubectl apply -f ~/istio-1.22.0/samples/addons/prometheus.yaml\n</code></pre>"},{"location":"obs-setup/#deploy-grafana","title":"Deploy Grafana","text":"<pre><code>kubectl apply -f ~/istio-1.22.0/samples/addons/grafana.yaml\n</code></pre>"},{"location":"obs-setup/#deploy-zipkin","title":"Deploy Zipkin","text":"<pre><code>kubectl apply -f ~/istio-1.22.0/samples/addons/extras/zipkin.yaml\n</code></pre>"},{"location":"obs-setup/#deploy-kiali","title":"Deploy Kiali","text":"<pre><code>kubectl apply -f ~/istio-1.22.0/samples/addons/kiali.yaml\n</code></pre> <p>Wait on each deployment to be ready.  Check on the workloads with:</p> <pre><code>kubectl get pod -n istio-system\n</code></pre>"},{"location":"obs-setup/#reconfigure-ingress-for-bookinfo","title":"Reconfigure ingress for <code>bookinfo</code>","text":"<p>Delete the ingress configuration to <code>httpbin</code>:</p> <pre><code>kubectl delete -f artifacts/data-plane/httpbin-gateway.yaml\n</code></pre> <p>In its place, configure ingress for <code>bookinfo</code>:</p> <pre><code>kubectl apply -f artifacts/mesh-config/bookinfo-gateway.yaml\n</code></pre> <p>Scale the productpage deployment back to one replica:</p> <pre><code>kubectl scale deploy productpage-v1 --replicas 1\n</code></pre> <p>Verify that ingress is functioning:</p> <pre><code>curl http://$GATEWAY_IP/productpage\n</code></pre>"},{"location":"obs-setup/#generate-a-load-against-bookinfo","title":"Generate a load against <code>bookinfo</code>","text":"<p>Many tools exist for sending traffic through a system; fortio is one.</p> <p>Here we will use a simple bash loop to send a slow, steady flow of requests through the ingress gateway:</p> <pre><code>bash -c \"while true; do curl --head http://$GATEWAY_IP/productpage; sleep 0.5; done\"\n</code></pre> <p>Leave this while loop running in its own terminal.</p>"},{"location":"obs-setup/#access-the-dashboards","title":"Access the dashboards","text":"<p>You will primarily work with Grafana, Zipkin and Kiali.</p> <p>Each dashboard can be accessed through the dashboard command.</p> <p>For example:</p> <pre><code>istioctl dashboard grafana\n</code></pre> <p>This should automatically cause a browser to open to the URL of the Grafana dashboard.</p> <p>You will find an Istio folder under \"Dashboards\" in the side navigation bar, containing a variety of pre-built monitoring dashboards.</p> <p>Go ahead and close the browser window and terminate the <code>istioctl dashboard</code> command (press Ctrl+C), and proceed to the next activity.</p>"},{"location":"setup/","title":"Setup","text":""},{"location":"setup/#kubernetes","title":"Kubernetes","text":"<p>Before we can begin exploring Istio debugging, we need a Kubernetes cluster.</p> <p>Provision a local Kubernetes cluster with k3d:</p> <pre><code>k3d cluster create my-k8s-cluster \\\n  --k3s-arg \"--disable=traefik@server:0\" \\\n  --port 80:80@loadbalancer \\\n  --port 443:443@loadbalancer\n</code></pre> <pre><code>kubectl config get-contexts\n</code></pre>"},{"location":"setup/#istio-distribution","title":"Istio distribution","text":"<p>Download a copy of the Istio distribution:</p> <pre><code>curl -L https://istio.io/downloadIstio | ISTIO_VERSION=1.21.2 sh -\n</code></pre> <p>Copy the file <code>istio-1.21.2/bin/istioctl</code> to your PATH.</p> <p>Verify that <code>istioctl</code> is in your PATH by running these commands:</p> <pre><code>istioctl help\n</code></pre> <p>And:</p> <pre><code>istioctl version\n</code></pre>"},{"location":"setup/#workshop-artifacts","title":"Workshop artifacts","text":"<p>Download all yaml artifacts referenced in all scenarios as a single .tgz file here.</p>"},{"location":"sidecar-injection/","title":"Sidecar Injection","text":""},{"location":"sidecar-injection/#objective","title":"Objective","text":"<p>To explore troubleshooting issues relating to sidecar injection in Istio.</p>"},{"location":"sidecar-injection/#introduction","title":"Introduction","text":"<p>The Istio documentation contains a guide on sidecar injection that explains the mechanism for configuring sidecar injection, and ways in which sidecar injection can be controlled.</p>"},{"location":"sidecar-injection/#main-lesson","title":"Main lesson","text":"<p>The most important thing to understand about sidecar injection is that it occurs at pod creation time.</p> <p>This means that if you configure a pod or a namespace for sidecar injection, it won't affect any pods that are already running.</p>"},{"location":"sidecar-injection/#example","title":"Example","text":"<p>Deploy the <code>httpbin</code> workload to the default namespace:</p> <pre><code>kubectl apply -f ~/istio-1.21.2/samples/httpbin/httpbin.yaml\n</code></pre> <p>The simplest way to tell whether sidecar injection took place is to display the number containers running in the pod:</p> <pre><code>kubectl get pod\n</code></pre> <p>Note the READY column shows 1/1 (one out of one) containers.</p> <p>Label the <code>default</code> namespace for sidecar injection:</p> <pre><code>kubectl label ns default istio-injection=enabled\n</code></pre> <p>The above act is passive:  nothing will happen until a pod is created in the <code>default</code> namespace.</p> <p>Either:</p> <ul> <li> <p>Delete the pod and let the deployment create a new one in its place:</p> <pre><code>kubectl delete pod -l app=httpbin\n</code></pre> </li> <li> <p>Submit a command to the kube API server to restart the deployment:</p> <pre><code>kubectl rollout restart deploy httpbin\n</code></pre> </li> </ul> <p>List the pods in the <code>default</code> namespace once more:</p> <pre><code>kubectl get pod\n</code></pre> <p>The READY column now shows 2/2 containers.  We have a proxy.</p>"},{"location":"sidecar-injection/#ways-to-specify-sidecar-injection","title":"Ways to specify sidecar injection","text":""},{"location":"sidecar-injection/#automatic-at-the-namespace-level","title":"Automatic at the namespace level","text":"<p>Above, we used automatic sidecar injection.  This means that deployment manifests do not need to be modified.  Instead, a mutating admission webhook is given the chance to mutate the deployment specification before it is applied to the Kubernetes cluster.</p> <p>Verify that the mutating webhook <code>istio-sidecar-injection</code> exists on the cluster:</p> <pre><code>kubectl get mutatingwebhookconfigurations\n</code></pre> <p>The template used by the sidecar injector is encoded in the ConfigMap named <code>istio-sidecar-injector</code> in the <code>istio-system</code> namespace:</p> <pre><code>kubectl get cm -n istio-system istio-sidecar-injector -o yaml\n</code></pre> <p>Automatic sidecar injection at the namespace level is convenient, and the recommended method.</p>"},{"location":"sidecar-injection/#automatic-at-the-pod-level","title":"Automatic at the pod level","text":"<p>Istio provides a mechanism to control sidecar injection at the pod level, by labeling the pod with <code>sidecar.istio.io/inject</code> with the value \"true\".</p> <p>Instead of labeling the namespace, we could have just applied the label to the <code>httpbin</code> workload.</p>"},{"location":"sidecar-injection/#manual","title":"Manual","text":"<p>The Istio CLI provides the <code>kube-inject</code> command to render the template against a deployment manifest.</p> <p>For example:</p> <pre><code>istioctl kube-inject -f ~/istio-1.21.2/samples/httpbin/httpbin.yaml &gt; injected.yaml\n</code></pre> <p>Inspect the generated file <code>injected.yaml</code> and confirm that the deployment resource now specifies two containers.</p> <p>Question</p> <p>What name does Istio use for the sidecar container?</p>"},{"location":"sidecar-injection/#troubleshooting","title":"Troubleshooting","text":"<p>The typical questions one should ask when troubleshooting sidecar injection include:</p> <ul> <li> <p>Was the namespace labeled for sidecar injection?</p> <p>This can be verified with:</p> <pre><code>kubectl get ns -Listio-injection\n</code></pre> </li> <li> <p>Was the deployment restarted after labeling the namespace?</p> <p>This is one of the more common reasons that the sidecar is not injected.</p> </li> </ul>"},{"location":"sidecar-injection/#how-many-proxies-does-istio-see","title":"How many proxies does Istio see?","text":"<p>Remember the <code>istioctl version</code> command?  Run it again. How many proxies are listed in the data plane? The original count was one. If the count still shows \"1\", it means that no additional proxies were deployed.</p>"},{"location":"sidecar-injection/#proxy-status","title":"Proxy status","text":"<p>Yet another diagnostic command is the proxy-status command</p> <pre><code>istioctl proxy-status\n</code></pre> <p>If <code>httpbin</code> is not listed in the output, it's an indication that Istio does not know about it, possibly because no sidecar was injected.</p> <p>We will revisit the <code>proxy-status</code> command later.</p>"},{"location":"sidecar-injection/#check-injection","title":"Check injection","text":"<p>The Istio CLI provides the check-inject command to check whether sidecar injection has taken place for a given workload:</p> <pre><code>istioctl x check-inject -l app=httpbin\n</code></pre> <p>Look for a green checkmark under the INJECTED column.</p>"},{"location":"sidecar-injection/#consequences-of-a-missing-sidecar","title":"Consequences of a missing sidecar","text":"<p>Fundamentally, a missing sidecar means that all traffic in and out of the pod will not be controlled by Istio.</p> <p>This implies that:</p> <ul> <li>Istio Custom Resources applied, targeting that workload, will have no effect, as there is no proxy to program.</li> <li>The workload will not be considered a part of the mesh.  No service discovery information will be communicated to peer workloads.</li> </ul> <p>In specific circumstances, it could mean that communication between the workload and other mesh workloads will not function.</p>"},{"location":"sidecar-injection/#example-scenario","title":"Example scenario","text":"<p>Remove the sidecar injection label from the namespace:</p> <pre><code>kubectl label ns default istio-injection-\n</code></pre> <p>Deploy the <code>sleep</code> sample, a convenience client from which to <code>curl</code> other workloads:</p> <pre><code>kubectl apply -f ~/istio-1.21.2/samples/sleep/sleep.yaml\n</code></pre> <p>Note that the <code>sleep</code> worklaod has no sidecar.</p> <p>Yet, we can still call <code>httpbin</code>:</p> <pre><code>kubectl exec deploy/sleep -- curl httpbin:8000/get\n</code></pre> <p>However, if our mesh was configured with strict mutual TLS Peer Authentication:</p> <pre><code>kubectl apply -f artifacts/injection/mtls-strict.yaml\n</code></pre> <p>An attempt to call <code>httpbin</code> once more will fail:</p> <pre><code>kubectl exec deploy/sleep -- curl httpbin:8000/get\n</code></pre> <p>The command fails with a <code>Connection reset by peer</code> error, because there is no forward proxy on <code>sleep</code> to upgrade the connection to mutual TLS, which is now a requirement.</p>"},{"location":"sidecar-injection/#sidecar-injection-problems","title":"Sidecar injection problems","text":"<p>The Istio docs have a page titled sidecar injection problems that catalogs sidecar injection errors and their remedies.</p>"},{"location":"sidecar-injection/#communication-between-proxies-and-istiod","title":"Communication between proxies and <code>istiod</code>","text":"<p>It is important to realize that <code>istiod</code> has a constant line of communication with all of the Envoy's:  both gateways and sidecars.</p> <p>When we run <code>istioctl proxy-status</code> we get insight into all the proxies that <code>istiod</code> controls, and whether the latest configurations have been sent and synced with each proxy.</p> <p>A common issue is forgetting to upgrade the sidecars after an Istio upgrade.</p> <p>To illustrate the issue, upgrade Istio in place.</p>"},{"location":"sidecar-injection/#upgrading-istio-and-dangling-sidecars","title":"Upgrading Istio and \"dangling\" sidecars","text":"<p>Download a newer version of Istio, version 1.22.0:</p> <pre><code>curl -L https://istio.io/downloadIstio | ISTIO_VERSION=1.22.0 sh -\n</code></pre> <p>Replace the <code>istioctl</code> CLI in your PATH with the one from the new distribution.</p> <p>Verify that the Istio CLI version is now version 1.22.0:</p> <pre><code>client version: 1.22.0\ncontrol plane version: 1.21.2\ndata plane version: 1.21.2 (2 proxies)\n</code></pre> <p>Upgrade Istio in-place:</p> <pre><code>istioctl upgrade -f artifacts/install/trace-config.yaml\n</code></pre> <p>Re-run <code>istioctl version</code> and note the output:</p> <pre><code>client version: 1.22.0\ncontrol plane version: 1.22.0\ndata plane version: 1.21.2 (1 proxies), 1.22.0 (1 proxies)\n</code></pre> <p>What is going on here?</p> <ol> <li>The Istio CLI was upgraded</li> <li>The control plane was also upgraded</li> <li>The ingress gateway component, running Envoy, was also upgraded</li> <li>However, note that there's still one proxy associated with the older version of Istio</li> </ol> <p>Get more information with:</p> <pre><code>istioctl proxy-status\n</code></pre> <p>We can see that <code>httpbin</code> was left alone.  It still has a sidecar, but it's not associated with the new control plane. It's a sort of \"orphaned\" sidecar in that the new, updated Istio is not communicating with it.</p> <p>After an upgrade, it's important to be aware that the sidecars (the data plane) are not updated automatically.</p> <p>To update <code>httpbin</code>'s sidecar, restart the deployment:</p> <pre><code>kubectl rollout restart deploy httpbin\n</code></pre> <p>Re-run <code>istioctl proxy-status</code> and verify that the <code>httpbin</code> sidecar is now associated with Istio version 1.22.0.</p>"}]}